{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('PyCapacitron': conda)",
   "metadata": {
    "interpreter": {
     "hash": "04ae8f06ef2069926318fb21ef951f475c5c242dceb49e7faf78b2393ada81ce"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "/home/big-boy/anaconda3/envs/PyCapacitron/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n",
      " > Using model: Tacotron\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:24000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:95.0\n",
      " | > mel_fmax:12000.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:None\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      "15675\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# this code is copied from: https://github.com/mozilla/TTS/blob/master/notebooks/Benchmark.ipynb\n",
    "#\n",
    "\n",
    "# TTS\n",
    "tts_pretrained_model = '/home/big-boy/Models/Blizzard/capacitron-April-06-2021_12+36PM-26e9ee0/best_model.pth.tar'\n",
    "tts_pretrained_model_config = '/home/big-boy/projects/Capacitron/TTS/tts/configs/capacitron_blizzard.json'\n",
    "\n",
    "import io\n",
    "import torch \n",
    "import time\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pylab as plt\n",
    "import IPython\n",
    "\n",
    "%pylab inline\n",
    "rcParams[\"figure.figsize\"] = (16,5)\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from TTS.tts.models.tacotron import Tacotron \n",
    "from TTS.tts.layers import *\n",
    "from TTS.tts.utils.data import *\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.utils.io import load_config \n",
    "from TTS.tts.utils.generic_utils import setup_model\n",
    "\n",
    "from TTS.tts.utils.text import text_to_sequence\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "from TTS.tts.utils.visual import visualize\n",
    "from TTS.tts.utils.text.symbols import symbols, phonemes\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "def tts(model, text, CONFIG, use_cuda, ap, use_gl, speaker_id=None, figures=True):\n",
    "    t_1 = time.time()\n",
    "    waveform, alignment, decoder_output, postnet_output, stop_tokens, inputs = synthesis(model, text, CONFIG, use_cuda, ap, truncated=True, enable_eos_bos_chars=CONFIG.enable_eos_bos_chars)\n",
    "    if CONFIG.model == \"Tacotron\" and not use_gl:\n",
    "        postnet_output = ap.out_linear_to_mel(postnet_output.T).T\n",
    "    if not use_gl:\n",
    "        waveform = wavernn.generate(torch.FloatTensor(postnet_output.T).unsqueeze(0).cuda(), batched=batched_wavernn, target=11000, overlap=550)\n",
    "\n",
    "    print(\" >  Run-time: {}\".format(time.time() - t_1))\n",
    "    if figures:                                                                                                         \n",
    "        visualize(alignment, postnet_output, stop_tokens, text, ap.hop_length, CONFIG, mel_spec)                                                                       \n",
    "    # IPython.display.display(Audio(waveform, rate=CONFIG.audio['sample_rate']))  \n",
    "    # OUT_FOLDER = 'out/'\n",
    "    # file_name = 'yolo.wav'\n",
    "    # os.makedirs(OUT_FOLDER, exist_ok=True)\n",
    "    # #file_name = text.replace(\" \", \"_\").replace(\".\",\"\") + \".wav\"\n",
    "    # out_path = os.path.join(OUT_FOLDER, file_name)\n",
    "    # ap.save_wav(waveform, out_path)\n",
    "    print(waveform)\n",
    "    return alignment, postnet_output, stop_tokens, waveform\n",
    "  \n",
    "use_cuda = True\n",
    "batched_wavernn = True\n",
    "\n",
    "# initialize TTS\n",
    "CONFIG = load_config(tts_pretrained_model_config)\n",
    "# load the model\n",
    "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
    "model = setup_model(num_chars, 1, CONFIG)\n",
    "# load the audio processor\n",
    "ap = AudioProcessor(**CONFIG.audio)         \n",
    "# load model state\n",
    "if use_cuda:\n",
    "    cp = torch.load(tts_pretrained_model)\n",
    "else:\n",
    "    cp = torch.load(tts_pretrained_model, map_location=lambda storage, loc: storage)\n",
    "\n",
    "# load the model\n",
    "model.load_state_dict(cp['model'])\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "print(cp['step'])\n",
    "model.decoder.max_decoder_steps = 2000\n",
    "\n",
    "# # initialize WaveRNN\n",
    "# VOCODER_CONFIG = load_config(wavernn_pretrained_model_config)\n",
    "# with localimport('/content/WaveRNN') as _importer:\n",
    "#   from models.wavernn import Model\n",
    "# bits = 10\n",
    "\n",
    "# wavernn = Model(\n",
    "#         rnn_dims=512,\n",
    "#         fc_dims=512,\n",
    "#         mode=\"mold\",\n",
    "#         pad=2,\n",
    "#         upsample_factors=VOCODER_CONFIG.upsample_factors,  # set this depending on dataset\n",
    "#         feat_dims=VOCODER_CONFIG.audio[\"num_mels\"],\n",
    "#         compute_dims=128,\n",
    "#         res_out_dims=128,\n",
    "#         res_blocks=10,\n",
    "#         hop_length=ap.hop_length,\n",
    "#         sample_rate=ap.sample_rate,\n",
    "#     ).cuda()\n",
    "# check = torch.load(wavernn_pretrained_model)\n",
    "# wavernn.load_state_dict(check['model'])\n",
    "# if use_cuda:\n",
    "#     wavernn.cuda()\n",
    "# wavernn.eval()\n",
    "# print(check['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (119) must match the existing size (128) at non-singleton dimension 1.  Target sizes: [1, 119, -1].  Tensor sizes: [128, 1]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06d3eac2ee1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSENTENCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Bill got in the habit of asking himself “Is that thought true?” And if he wasn’t absolutely certain it was, he just let it go.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-f2283d96423b>\u001b[0m in \u001b[0;36mtts\u001b[0;34m(model, text, CONFIG, use_cuda, ap, use_gl, speaker_id, figures)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mt_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostnet_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_eos_bos_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eos_bos_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Tacotron\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_gl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mpostnet_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_linear_to_mel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostnet_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/Capacitron/TTS/tts/utils/synthesis.py\u001b[0m in \u001b[0;36msynthesis\u001b[0;34m(model, text, CONFIG, use_cuda, ap, speaker_id, style_wav, reference_wav, truncated, enable_eos_bos_chars, use_griffin_lim, do_trim_silence, speaker_embedding, backend)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# synthesize voice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'torch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         decoder_output, postnet_output, alignments, stop_tokens = run_model_torch(\n\u001b[0m\u001b[1;32m    288\u001b[0m             model, inputs, CONFIG, truncated, speaker_id, style_mel, reference_mel, speaker_embeddings=speaker_embedding)\n\u001b[1;32m    289\u001b[0m         postnet_output, decoder_output, alignment, stop_tokens = parse_outputs_torch(\n",
      "\u001b[0;32m~/projects/Capacitron/TTS/tts/utils/synthesis.py\u001b[0m in \u001b[0;36mrun_model_torch\u001b[0;34m(model, inputs, CONFIG, truncated, speaker_id, style_mel, reference_mel, speaker_embeddings)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 inputs, style_mel=style_mel, speaker_ids=speaker_id, speaker_embeddings=speaker_embeddings)\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_capacitron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             decoder_output, postnet_output, alignments, stop_tokens = model.inference(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 inputs, reference_mel=reference_mel, speaker_ids=speaker_id, speaker_embeddings=speaker_embeddings)\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyCapacitron/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/Capacitron/TTS/tts/models/tacotron.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, characters, speaker_ids, style_mel, reference_mel, speaker_embeddings)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapacitron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# B x capacitron_VAE_embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             encoder_outputs = self.compute_VAE_embedding(encoder_outputs,\n\u001b[0m\u001b[1;32m    233\u001b[0m                                                          \u001b[0mreference_mel_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreference_mel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreference_mel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                                                          \u001b[0mtext_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapacitron_use_text_summary_embeddings\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/Capacitron/TTS/tts/models/tacotron_abstract.py\u001b[0m in \u001b[0;36mcompute_VAE_embedding\u001b[0;34m(self, inputs, reference_mel_info, text_info, speaker_embedding)\u001b[0m\n\u001b[1;32m    250\u001b[0m                                                                                                              \u001b[0mtext_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                                                                                                              speaker_embedding)  # pylint: disable=not-callable\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_speaker_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAE_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# concatenate to the output of the basic tacotron encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacitron_beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/Capacitron/TTS/tts/models/tacotron_abstract.py\u001b[0m in \u001b[0;36m_concat_speaker_embedding\u001b[0;34m(outputs, speaker_embeddings)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_concat_speaker_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         speaker_embeddings_ = speaker_embeddings.expand(\n\u001b[0m\u001b[1;32m    265\u001b[0m             outputs.size(0), outputs.size(1), -1)\n\u001b[1;32m    266\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_embeddings_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (119) must match the existing size (128) at non-singleton dimension 1.  Target sizes: [1, 119, -1].  Tensor sizes: [128, 1]"
     ]
    }
   ],
   "source": [
    "SENTENCE = 'Bill got in the habit of asking himself “Is that thought true?” And if he wasn’t absolutely certain it was, he just let it go.'\n",
    "align, spec, stop_tokens, wav = tts(model, SENTENCE, CONFIG, use_cuda, ap, speaker_id=0, use_gl=True, figures=False)"
   ]
  }
 ]
}