{
    "github_branch":"* main",
    "restore_path":"/home/thorsten/___prj/tts/models/vocoder/fullband-melgan-main/output/thorsten-dca-fullband-melgan-main-branch-May-11-2021_02+26PM-0ee3eee/checkpoint_500000.pth.tar",
    "github_branch":"* main",
    "restore_path":"/home/thorsten/___prj/tts/models/vocoder/fullband-melgan-main/output/thorsten-dca-fullband-melgan-main-branch-May-06-2021_10+34AM-0ee3eee/checkpoint_400000.pth.tar",
    "github_branch":"* main",
    "restore_path":"/home/thorsten/___prj/tts/models/vocoder/fullband-melgan-main/output/thorsten-dca-fullband-melgan-main-branch-May-01-2021_06+07AM-0ee3eee/checkpoint_300000.pth.tar",
    "github_branch":"* main",
    "restore_path":"/home/thorsten/___prj/tts/models/vocoder/fullband-melgan-main/output/thorsten-dca-fullband-melgan-main-branch-April-28-2021_10+55PM-0ee3eee/checkpoint_200000.pth.tar",
    "github_branch":"* main",
    "restore_path":"/home/thorsten/___prj/tts/models/vocoder/fullband-melgan-main/output/thorsten-dca-fullband-melgan-main-branch-April-26-2021_03+42PM-0ee3eee/checkpoint_100000.pth.tar",
    "github_branch":"* main",
    "github_branch":"* main",
        "run_name": "thorsten-dca-fullband-melgan-main-branch",
        "run_description": "FullBand-MelGAN Vocoder on thorsten v03 vocoder dataset. Commit id 0ee3eeefb553678d56c49534f3972a426a254649",
    
        // AUDIO PARAMETERS
        "audio":{
            "fft_size": 1024,         // number of stft frequency levels. Size of the linear spectogram frame.
            "win_length": 1024,      // stft window length in ms.
            "hop_length": 256,       // stft window hop-lengh in ms.
            "frame_length_ms": null, // stft window length in ms.If null, 'win_length' is used.
            "frame_shift_ms": null,  // stft window hop-lengh in ms. If null, 'hop_length' is used.
    
            // Audio processing parameters
            "sample_rate": 22050,   // DATASET-RELATED: wav sample-rate. If different than the original data, it is resampled.
            "preemphasis": 0.0,     // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.
            "ref_level_db": 20,     // reference level db, theoretically 20db is the sound of air.
    
            // Silence trimming
            "do_trim_silence": false,// enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)
            "trim_db": 50,          // threshold for timming silence. Set this according to your dataset.
    
            // MelSpectrogram parameters
            "num_mels": 80,         // size of the mel spec frame.
            "mel_fmin": 50.0,        // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!
            "mel_fmax": 8000.0,     // maximum freq level for mel-spec. Tune for dataset!!
            "spec_gain": 1.0,         // scaler value appplied after log transform of spectrogram.
    
            // Normalization parameters
            "signal_norm": true,    // normalize spec values. Mean-Var normalization if 'stats_path' is defined otherwise range normalization defined by the other params.
            "min_level_db": -100,   // lower bound for normalization
            "symmetric_norm": true, // move normalization to range [-1, 1]
            "max_norm": 4.0,        // scale normalization to range [-max_norm, max_norm] or [0, max_norm]
            "clip_norm": true,      // clip normalized values into the range.
            "stats_path": "/home/thorsten/___prj/tts/models/taco2/thorsten-dca/spec-stats.npy"    // DO NOT USE WITH MULTI_SPEAKER MODEL. scaler stats file computed by 'compute_statistics.py'. If it is defined, mean-std based notmalization is used and other normalization params are ignored
        },
    
        // DISTRIBUTED TRAINING
        // "distributed":{
        //     "backend": "nccl",
        //     "url": "tcp:\/\/localhost:54321"
        // },
    
        // MODEL PARAMETERS
        "use_pqmf": false,
    
        // LOSS PARAMETERS
        "use_stft_loss": true,
        "use_subband_stft_loss": false,
        "use_mse_gan_loss": true,
        "use_hinge_gan_loss": false,
        "use_feat_match_loss": true,  // use only with melgan discriminators
        "use_l1_spec_loss": true,
    
        // loss weights
        "stft_loss_weight": 0,
        "subband_stft_loss_weight": 0,
        "mse_G_loss_weight": 1,
        "hinge_G_loss_weight": 0,
        "feat_match_loss_weight": 108,
        "l1_spec_loss_weight": 45,
    
        // multiscale stft loss parameters
        "stft_loss_params": {
            "n_ffts": [1024, 2048, 512],
            "hop_lengths": [120, 240, 50],
            "win_lengths": [600, 1200, 240]
        },
    
        "l1_spec_loss_params": {
            "use_mel": true,
            "sample_rate": 22050,
            "n_fft": 1024,
            "hop_length": 256,
            "win_length": 1024,
            "n_mels": 80,
            "mel_fmin": 50.0,
            "mel_fmax": 8000.0
        },
    
        "target_loss": "avg_G_loss",  // loss value to pick the best model to save after each epoch
    
        // DISCRIMINATOR
        "discriminator_model": "melgan_multiscale_discriminator",
        "discriminator_model_params":{
            "base_channels": 16,
            "max_channels":512,
            "downsample_factors":[4, 4, 4]
        },
        "steps_to_start_discriminator": 200000,      // steps required to start GAN trainining.1
        "diff_samples_for_G_and_D": true,
    
        // GENERATOR
        "generator_model": "fullband_melgan_generator",
        "generator_model_params": {
            "upsample_factors":[8, 8, 4],
            "num_res_blocks": 4
        },
    
        // DATASET
        "data_path": "/home/thorsten/___prj/tts/datasets/thorsten-de_v03/wavs/",
        "feature_path": "/home/thorsten/___prj/tts/datasets/thorsten-de_v03/features/",
        "seq_len": 16384,
        "pad_short": 2000,
        "conv_pad": 0,
        "use_noise_augment": false,
        "use_cache": true,
    
        "reinit_layers": [],    // give a list of layer names to restore from the given checkpoint. If not defined, it reloads all heuristically matching layers.
    
        // TRAINING
        "batch_size": 48,       // Batch size for training. Lower values than 32 might cause hard to learn attention. It is overwritten by 'gradual_training'.
    
        // VALIDATION
        "run_eval": true,
        "test_delay_epochs": 10,  //Until attention is aligned, testing only wastes computation time.
        "test_sentences_file": "/home/thorsten/___prj/tts/models/taco2/thorsten-dca/test_sentences.txt",  // set a file to load sentences to be used for testing. If it is null then we use default english sentences.
    
        // OPTIMIZER
        "epochs": 10000,                // total number of epochs to train.
        "wd": 0.0,                // Weight decay weight.
        "gen_clip_grad": -1,      // Generator gradient clipping threshold. Apply gradient clipping if > 0
        "disc_clip_grad": -1,     // Discriminator gradient clipping threshold.
        "lr_scheduler_gen": "MultiStepLR",   // one of the schedulers from https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
        "lr_scheduler_gen_params": {
            "gamma": 0.5,
            "milestones": [100000, 200000, 300000, 400000, 500000, 600000, 700000, 800000]
        },
        "lr_scheduler_disc": "MultiStepLR",   // one of the schedulers from https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
        "lr_scheduler_disc_params": {
               "gamma": 0.5,
              "milestones": [100000, 200000, 300000, 400000, 500000, 600000, 700000, 800000]
        },
        "lr_gen": 0.000003125,                  // Initial learning rate. If Noam decay is active, maximum learning rate.
        "lr_disc": 0.000003125,
        "optimizer": "AdamW",
        "optimizer_params":{
            "betas": [0.8, 0.99],
            "weight_decay": 0.0
        },
    
        // TENSORBOARD and LOGGING
        "print_step": 25,       // Number of steps to log traning on console.
        "print_eval": false,     // If True, it prints loss values for each step in eval run.
        "save_step": 25000,      // Number of training steps expected to plot training stats on TB and save model checkpoints.
        "checkpoint": true,     // If true, it saves checkpoints per "save_step"
        "tb_model_param_stats": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging.
    
        // DATA LOADING
        "num_loader_workers": 4,        // number of training data loader processes. Don't set it too big. 4-8 are good values.
        "num_val_loader_workers": 4,    // number of evaluation data loader processes.
        "eval_split_size": 10,
    
        // PATHS
        "output_path": "/home/thorsten/___prj/tts/models/vocoder/fullband-melgan-main/output"
    }
    
    
    